{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FARM-MTL-Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwhgI5Xi7upW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c60f4b-62a4-4be0-e0c0-aa3544586796"
      },
      "source": [
        "!pip install utils/FARM --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 51.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.2MB 19.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 43.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 327kB 46.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5MB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 46.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6MB 32.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 283kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 37.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 58.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 36.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 34.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 54.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25h  Building wheel for farm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.84 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhJA6JNe7xha"
      },
      "source": [
        "# Importing common utilities\n",
        "import ast\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLQP1-bq8Kmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e395fbbb-e855-4e0f-8c66-928ace1d9187"
      },
      "source": [
        "# FARM imports utils\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead, TokenClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import set_all_seeds, initialize_device_settings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/30/2021 23:58:21 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTL7Bdma4SlR"
      },
      "source": [
        "from farm.data_handler.processor import Processor\n",
        "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
        "from farm.data_handler.samples import (\n",
        "    Sample,\n",
        "    SampleBasket,\n",
        ")\n",
        "from farm.data_handler.utils import expand_labels\n",
        "\n",
        "class MTLProcessor(Processor):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        max_seq_len,\n",
        "        data_dir,\n",
        "        train_filename,\n",
        "        test_filename,\n",
        "        delimiter,\n",
        "        dev_split=0.0,\n",
        "        dev_filename=None,\n",
        "        label_list=None,\n",
        "        metric=None,\n",
        "        proxies=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "        super(MTLProcessor, self).__init__(\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_len=max_seq_len,\n",
        "            train_filename=train_filename,\n",
        "            dev_filename=dev_filename,\n",
        "            test_filename=test_filename,\n",
        "            dev_split=dev_split,\n",
        "            data_dir=data_dir,\n",
        "            tasks={},\n",
        "            proxies=proxies\n",
        "        )\n",
        "\n",
        "    def file_to_dicts(self, file: str) -> [dict]:\n",
        "      dicts = list()\n",
        "      df = pd.read_csv(file)\n",
        "      for text, label, tokens in zip(df.post_tokens.values, df.post_label.values, df.toxic_tokens.values):\n",
        "        columns = dict()\n",
        "        text = ast.literal_eval(text)\n",
        "        tokens = ast.literal_eval(tokens)\n",
        "        columns[\"text\"] = \" \".join(text)\n",
        "        columns[\"document_level_task_label\"] = label # Key hard-coded\n",
        "        columns[\"token_level_task_label\"] = list(map(str, tokens)) # Key hard-coded\n",
        "        dicts.append(columns)\n",
        "      return dicts\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_start_of_word(word_ids):\n",
        "        words = np.array(word_ids)\n",
        "        words[words == None] = -1\n",
        "        start_of_word_single = [0] + list(np.ediff1d(words) > 0)\n",
        "        start_of_word_single = [int(x) for x in start_of_word_single]\n",
        "        return start_of_word_single\n",
        "\n",
        "    # Most of the code is copied from NERProcessor - dataset_from_dicts()\n",
        "    def dataset_from_dicts(self, dicts, indices=None, return_baskets=False, non_initial_token=\"X\"):\n",
        "      self.baskets = []\n",
        "      self.pre_tokenizer = WhitespaceSplit()\n",
        "\n",
        "      texts = [x[\"text\"] for x in dicts]\n",
        "      words_and_spans = [self.pre_tokenizer.pre_tokenize_str(x) for x in texts]\n",
        "      words = [[x[0] for x in y] for y in words_and_spans]\n",
        "\n",
        "      word_spans_batch = [[x[1] for x in y] for y in words_and_spans]\n",
        "\n",
        "      tokenized_batch = self.tokenizer.batch_encode_plus(\n",
        "          words,\n",
        "          return_offsets_mapping=True,\n",
        "          return_special_tokens_mask=True,\n",
        "          return_token_type_ids=True,\n",
        "          return_attention_mask=True,\n",
        "          truncation=True,\n",
        "          max_length=self.max_seq_len,\n",
        "          padding=\"max_length\",\n",
        "          is_split_into_words=True,\n",
        "      )\n",
        "\n",
        "      for i in range(len(dicts)):\n",
        "          tokenized = tokenized_batch[i]\n",
        "          d = dicts[i]\n",
        "          id_external = self._id_from_dict(d)\n",
        "          if indices:\n",
        "              id_internal = indices[i]\n",
        "          else:\n",
        "              id_internal = i\n",
        "\n",
        "          input_ids = tokenized.ids\n",
        "          segment_ids = tokenized.type_ids\n",
        "          initial_mask = self._get_start_of_word(tokenized.words)\n",
        "          assert len(initial_mask) == len(input_ids)\n",
        "\n",
        "          padding_mask = tokenized.attention_mask\n",
        "\n",
        "          if return_baskets:\n",
        "              token_to_word_map = tokenized.words\n",
        "              word_spans = word_spans_batch[i]\n",
        "              tokenized_dict = {\n",
        "                  \"tokens\": tokenized.tokens,\n",
        "                  \"word_spans\": word_spans,\n",
        "                  \"token_to_word_map\": token_to_word_map,\n",
        "                  \"start_of_word\": initial_mask\n",
        "              }\n",
        "          else:\n",
        "              tokenized_dict = {}\n",
        "\n",
        "          feature_dict = {\n",
        "              \"input_ids\": input_ids,\n",
        "              \"padding_mask\": padding_mask,\n",
        "              \"segment_ids\": segment_ids,\n",
        "              \"initial_mask\": initial_mask,\n",
        "          }\n",
        "\n",
        "          for task_name, task in self.tasks.items():\n",
        "              try:\n",
        "                  label_name = task[\"label_name\"]\n",
        "                  labels_word = d[label_name]\n",
        "                  label_list = task[\"label_list\"]\n",
        "                  label_tensor_name = task[\"label_tensor_name\"]\n",
        "\n",
        "                  if task[\"task_type\"] == \"classification\":\n",
        "                      label_ids = [label_list.index(labels_word)]\n",
        "                  elif task[\"task_type\"] == \"ner\":\n",
        "                      labels_token = expand_labels(labels_word, initial_mask, non_initial_token)\n",
        "                      label_ids = [label_list.index(lt) for lt in labels_token]\n",
        "              except ValueError:\n",
        "                  label_ids = None\n",
        "                  problematic_labels = set(labels_token).difference(set(label_list))\n",
        "                  print(f\"[Task: {task_name}] Could not convert labels to ids via label_list!\"\n",
        "                                  f\"\\nWe found a problem with labels {str(problematic_labels)}\")\n",
        "              except KeyError:\n",
        "                  label_ids = None\n",
        "              if label_ids:\n",
        "                  feature_dict[label_tensor_name] = label_ids\n",
        "\n",
        "          curr_sample = Sample(id=None,\n",
        "                                  clear_text=d,\n",
        "                                  tokenized=tokenized_dict,\n",
        "                                  features=[feature_dict])\n",
        "          curr_basket = SampleBasket(id_internal=id_internal,\n",
        "                                      raw=d,\n",
        "                                      id_external=id_external,\n",
        "                                      samples=[curr_sample])\n",
        "          self.baskets.append(curr_basket)\n",
        "\n",
        "      if indices and 0 not in indices:\n",
        "          pass\n",
        "      else:\n",
        "          self._log_samples(1)\n",
        "\n",
        "      dataset, tensor_names = self._create_dataset()\n",
        "      ret = [dataset, tensor_names, self.problematic_sample_ids]\n",
        "      if return_baskets:\n",
        "          ret.append(self.baskets)\n",
        "      return tuple(ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNx-LKQW4ZoI"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def custom_f1_score(y_true, y_pred):\n",
        "  f1_scores = []\n",
        "  for t, p in zip(y_true, y_pred):\n",
        "    f1_scores.append(f1_score(t, p, average='macro'))\n",
        "  return {\"f1 macro score\" : sum(f1_scores) / len(f1_scores), \"total\" : len(f1_scores)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0We5cf005e6x"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def mtl_loss_agg(individual_losses: List[torch.Tensor], global_step=None, batch=None):\n",
        "    loss = torch.sum(individual_losses[0]) + torch.sum(individual_losses[1])\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeyL8--_8NfN",
        "outputId": "76980a67-a3c0-4355-b576-170359d6a29d"
      },
      "source": [
        "DO_LOWER_CASE = False\n",
        "LANG_MODEL = \"bert-base-uncased\"\n",
        "TRAIN_FILE = \"/datasets/hatexplain_train.csv\"\n",
        "DEV_FILE = \"/datasets/hatexplain_dev.csv\"\n",
        "TEST_FILE = \"datasets/hatexplain_test.csv\"\n",
        "MAX_SEQ_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "N_EPOCHS = 1\n",
        "EMBEDS_DROPOUT_PROB = 0.1\n",
        "EVALUATE_EVERY = 500\n",
        "random_seed_list = [3, 42]\n",
        "DEVICE, N_GPU = initialize_device_settings(use_cuda=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/30/2021 23:58:36 - INFO - farm.utils -   Using device: CUDA \n",
            "05/30/2021 23:58:36 - INFO - farm.utils -   Number of GPUs: 1\n",
            "05/30/2021 23:58:36 - INFO - farm.utils -   Distributed Training: False\n",
            "05/30/2021 23:58:36 - INFO - farm.utils -   Automatic Mixed Precision: None\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzoOy7e44tGa"
      },
      "source": [
        "test_result_data = pd.read_csv(\"/datasets/hatexplain_test.csv\", delimiter=\",\")\n",
        "test_texts = []\n",
        "for idx, text in enumerate(test_result_data.post_tokens.values):\n",
        "  in_dict = {}\n",
        "  text = ast.literal_eval(text)\n",
        "  in_dict[\"text\"] = \" \".join(text)\n",
        "  test_texts.append(in_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf7L26jH4uuz",
        "outputId": "8c8ab1b7-9b88-4dbe-edc8-957990d8af83"
      },
      "source": [
        "LANG_MODEL, random_seed_list, N_EPOCHS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bert-base-uncased', [3, 42], 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3SYC8xZ4wmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4668ab11-c26a-417b-d398-d6ba732536ed"
      },
      "source": [
        "for random_seed in random_seed_list:\n",
        "  # Clean up\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Set the random seed\n",
        "  from farm.utils import set_all_seeds\n",
        "  set_all_seeds(seed=random_seed)\n",
        "\n",
        "  !rm -rf /content/early-stopping-model \n",
        "\n",
        "  tokenizer = Tokenizer.load(\n",
        "    pretrained_model_name_or_path=LANG_MODEL,\n",
        "    do_lower_case=DO_LOWER_CASE,\n",
        "    # add_prefix_space=True, # For roberta only\n",
        "    )\n",
        "  \n",
        "  NER_LABELS = [\"X\", \"0\", \"1\"]\n",
        "  LABEL_LIST = [\"normal\", \"offensive\", \"hatespeech\"]\n",
        "\n",
        "  processor = MTLProcessor(data_dir = \".\", \n",
        "                              tokenizer=tokenizer,\n",
        "                              max_seq_len=128,\n",
        "                              train_filename=TRAIN_FILE,\n",
        "                              test_filename=TEST_FILE,\n",
        "                              dev_filename=DEV_FILE,\n",
        "                              delimiter=\",\",\n",
        "                              )\n",
        "  \n",
        "\n",
        "  from farm.evaluation.metrics import register_metrics\n",
        "  register_metrics('f1_weighted', custom_f1_score)\n",
        "\n",
        "  metric = 'f1_weighted'\n",
        "  processor.add_task(name=\"document_level_task\", label_list=LABEL_LIST, metric=\"acc\", text_column_name=\"text\", label_column_name=\"label\", task_type=\"classification\")\n",
        "  processor.add_task(name=\"token_level_task\", label_list=NER_LABELS, metric=metric, text_column_name=\"text\", label_column_name=\"tokens\", task_type=\"ner\")\n",
        "\n",
        "  data_silo = DataSilo(processor=processor,\n",
        "                      batch_size=BATCH_SIZE\n",
        "                      )\n",
        "  \n",
        "\n",
        "  from farm.train import EarlyStopping\n",
        "  from pathlib import Path\n",
        "\n",
        "  earlystopping = EarlyStopping(\n",
        "                                metric=\"loss\", mode=\"min\",\n",
        "                                save_dir=Path(\"./early-stopping-model\"),\n",
        "                                patience=10\n",
        "                               )\n",
        "  \n",
        "  language_model = LanguageModel.load(LANG_MODEL)\n",
        "\n",
        "  document_level_task_head = TextClassificationHead(num_labels=len(LABEL_LIST), task_name=\"document_level_task\")\n",
        "  token_level_task_head = TokenClassificationHead(num_labels=len(NER_LABELS), task_name=\"token_level_task\")\n",
        "\n",
        "  model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[document_level_task_head, token_level_task_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\", \"per_token\"],\n",
        "    device=DEVICE,\n",
        "    loss_aggregation_fn=mtl_loss_agg)\n",
        "  \n",
        "  model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=DEVICE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "\n",
        "  trainer = Trainer(model=model,\n",
        "                    optimizer=optimizer,\n",
        "                    data_silo=data_silo,\n",
        "                    epochs=N_EPOCHS,\n",
        "                    n_gpu=N_GPU,\n",
        "                    lr_schedule=lr_schedule,\n",
        "                    device=DEVICE,\n",
        "                    evaluate_every=EVALUATE_EVERY,\n",
        "                    # early_stopping=earlystopping,\n",
        "                    )\n",
        "\n",
        "  model = trainer.train()\n",
        "\n",
        "  from pathlib import Path\n",
        "  save_dir = Path(\"/content/early-stopping-model\")\n",
        "\n",
        "  model.save(save_dir)\n",
        "  processor.save(save_dir)\n",
        "\n",
        "  from farm.infer import Inferencer\n",
        "\n",
        "  model = Inferencer.load(save_dir, gpu=True)\n",
        "  result = model.inference_from_dicts(dicts=test_texts)\n",
        "\n",
        "  label_predictions_list, tokens_predictions_list = [], []\n",
        "  for idx, chunk_res in enumerate(result):\n",
        "    if idx % 2 == 0:\n",
        "      label_predictions_list += chunk_res['predictions']\n",
        "    else:\n",
        "      tokens_predictions_list += chunk_res['predictions']\n",
        "\n",
        "  # Tokens\n",
        "  tokens_list = []\n",
        "  for idx, pred_ind_list in enumerate(tokens_predictions_list):\n",
        "    ind_list = []\n",
        "    for val_dict in pred_ind_list:\n",
        "      label_val = val_dict['label']\n",
        "      ind_list.append(0 if label_val == 'X' else int(label_val))\n",
        "    tokens_list.append(ind_list)\n",
        "  test_result_data[\"seed_token\" + str(random_seed)] = tokens_list \n",
        "\n",
        "  # Labels\n",
        "  label_list = []\n",
        "  for idx, pred_dict in enumerate(label_predictions_list):\n",
        "    label_list.append(pred_dict['label'])\n",
        "  test_result_data[\"seed_post\" + str(random_seed)] = label_list\n",
        "\n",
        "  # Clean up\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  print(\"Completed:\", \"seed_post\" + str(random_seed))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train epoch 0/0 (Cur. train loss: 144.3164): 100%|██████████| 361/361 [02:24<00:00,  2.51it/s]\n",
            "Evaluating: 100%|██████████| 121/121 [00:20<00:00,  5.78it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 361 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   \n",
            " _________ document_level_task _________\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   loss: 0.7300639125236488\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   task_name: document_level_task\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   acc: 0.6857440166493236\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      normal     0.7603    0.7046    0.7314      1598\n",
            "   offensive     0.5272    0.5118    0.5194      1061\n",
            "  hatespeech     0.7254    0.8160    0.7681      1185\n",
            "\n",
            "    accuracy                         0.6857      3844\n",
            "   macro avg     0.6710    0.6775    0.6729      3844\n",
            "weighted avg     0.6852    0.6857    0.6842      3844\n",
            "\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   \n",
            " _________ token_level_task _________\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   loss: 4.205996062827532\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   task_name: token_level_task\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   f1 macro score: 0.8072743192790377\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   total: 3844\n",
            "05/31/2021 00:02:57 - INFO - farm.eval -   report: \n",
            " Error\n",
            "05/31/2021 00:02:59 - INFO - farm.utils -   Using device: CUDA \n",
            "05/31/2021 00:02:59 - INFO - farm.utils -   Number of GPUs: 1\n",
            "05/31/2021 00:02:59 - INFO - farm.utils -   Distributed Training: False\n",
            "05/31/2021 00:02:59 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "05/31/2021 00:02:59 - INFO - farm.modeling.language_model -   \n",
            "05/31/2021 00:02:59 - INFO - farm.modeling.language_model -   LOADING MODEL\n",
            "05/31/2021 00:02:59 - INFO - farm.modeling.language_model -   =============\n",
            "05/31/2021 00:02:59 - INFO - farm.modeling.language_model -   Model found locally at /content/early-stopping-model\n",
            "05/31/2021 00:03:01 - INFO - farm.modeling.language_model -   Loaded /content/early-stopping-model\n",
            "05/31/2021 00:03:01 - INFO - farm.modeling.adaptive_model -   Found files for loading 2 prediction heads\n",
            "05/31/2021 00:03:01 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
            "05/31/2021 00:03:01 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 3]\n",
            "05/31/2021 00:03:01 - INFO - farm.modeling.prediction_head -   Loading prediction head from /content/early-stopping-model/prediction_head_0.bin\n",
            "05/31/2021 00:03:01 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
            "05/31/2021 00:03:01 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 3]\n",
            "05/31/2021 00:03:01 - INFO - farm.modeling.prediction_head -   Loading prediction head from /content/early-stopping-model/prediction_head_1.bin\n",
            "05/31/2021 00:03:01 - WARNING - farm.utils -   Failed to log params: Changing param values is not allowed. Param with key='lm_name' was already logged with value='bert-base-uncased' for run ID='da80393332de411caa1974f6c9e38e06'. Attempted logging new value '/content/early-stopping-model'.\n",
            "05/31/2021 00:03:01 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizerFast'\n",
            "05/31/2021 00:03:02 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
            "05/31/2021 00:03:02 - INFO - farm.utils -   Using device: CUDA \n",
            "05/31/2021 00:03:02 - INFO - farm.utils -   Number of GPUs: 1\n",
            "05/31/2021 00:03:02 - INFO - farm.utils -   Distributed Training: False\n",
            "05/31/2021 00:03:02 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "05/31/2021 00:03:02 - INFO - farm.infer -   Got ya 2 parallel workers to do inference ...\n",
            "05/31/2021 00:03:02 - INFO - farm.infer -    0    0 \n",
            "05/31/2021 00:03:02 - INFO - farm.infer -   /w\\  /w\\\n",
            "05/31/2021 00:03:02 - INFO - farm.infer -   /'\\  / \\\n",
            "05/31/2021 00:03:02 - INFO - farm.infer -     \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:101: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:101: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "05/31/2021 00:03:02 - INFO - farm.data_handler.processor -   *** Show 1 random examples ***\n",
            "05/31/2021 00:03:02 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: None\n",
            "Clear Text: \n",
            " \ttext: <user> <user> <user> i do not understand the myth of people who are against the death penalty but usually urge the govt to give the death penalty to rape culprits i am not saying to you but to political leader namely bay\n",
            "Tokenized: \n",
            " \ttokens: ['[CLS]', '<', 'user', '>', '<', 'user', '>', '<', 'user', '>', 'i', 'do', 'not', 'understand', 'the', 'myth', 'of', 'people', 'who', 'are', 'against', 'the', 'death', 'penalty', 'but', 'usually', 'urge', 'the', 'govt', 'to', 'give', 'the', 'death', 'penalty', 'to', 'rape', 'cu', '##lp', '##rit', '##s', 'i', 'am', 'not', 'saying', 'to', 'you', 'but', 'to', 'political', 'leader', 'namely', 'bay', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            " \tword_spans: [(0, 6), (7, 13), (14, 20), (21, 22), (23, 25), (26, 29), (30, 40), (41, 44), (45, 49), (50, 52), (53, 59), (60, 63), (64, 67), (68, 75), (76, 79), (80, 85), (86, 93), (94, 97), (98, 105), (106, 110), (111, 114), (115, 119), (120, 122), (123, 127), (128, 131), (132, 137), (138, 145), (146, 148), (149, 153), (154, 162), (163, 164), (165, 167), (168, 171), (172, 178), (179, 181), (182, 185), (186, 189), (190, 192), (193, 202), (203, 209), (210, 216), (217, 220)]\n",
            " \ttoken_to_word_map: [None, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 29, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            " \tstart_of_word: [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Features: \n",
            " \tinput_ids: [101, 1026, 5310, 1028, 1026, 5310, 1028, 1026, 5310, 1028, 1045, 2079, 2025, 3305, 1996, 10661, 1997, 2111, 2040, 2024, 2114, 1996, 2331, 6531, 2021, 2788, 9075, 1996, 22410, 2000, 2507, 1996, 2331, 6531, 2000, 9040, 12731, 14277, 14778, 2015, 1045, 2572, 2025, 3038, 2000, 2017, 2021, 2000, 2576, 3003, 8419, 3016, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:12<00:00,  8.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  9.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  9.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  9.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.99 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  9.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  9.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.99 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.99 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 95/95 [00:10<00:00,  8.96 Batches/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Completed: seed_post3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 00:04:53 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
            "05/31/2021 00:04:55 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "05/31/2021 00:04:55 - INFO - farm.data_handler.data_silo -   LOADING TRAIN DATA\n",
            "05/31/2021 00:04:55 - INFO - farm.data_handler.data_silo -   ==================\n",
            "05/31/2021 00:04:55 - INFO - farm.data_handler.data_silo -   Loading train set from: /content/hatexplain_train.csv \n",
            "05/31/2021 00:04:56 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 11535 dictionaries to pytorch datasets (chunksize = 1154)...\n",
            "05/31/2021 00:04:56 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "05/31/2021 00:04:56 - INFO - farm.data_handler.data_silo -   /w\\  /w\\\n",
            "05/31/2021 00:04:56 - INFO - farm.data_handler.data_silo -   /'\\  / \\\n",
            "05/31/2021 00:04:56 - INFO - farm.data_handler.data_silo -     \n",
            "Preprocessing Dataset /content/hatexplain_train.csv:   0%|          | 0/11535 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "05/31/2021 00:04:56 - INFO - farm.data_handler.processor -   *** Show 1 random examples ***\n",
            "05/31/2021 00:04:56 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: None\n",
            "Clear Text: \n",
            " \ttext: xiaxue hey dan do you like apples dan yeah i like apples xiaxue dan raped me dan wtf i did not rape you xiaxue how do you like them apples\n",
            " \tdocument_level_task_label: normal\n",
            " \ttoken_level_task_label: ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
            "Tokenized: \n",
            " \tNone\n",
            "Features: \n",
            " \tinput_ids: [101, 8418, 8528, 5657, 4931, 4907, 2079, 2017, 2066, 18108, 4907, 3398, 1045, 2066, 18108, 8418, 8528, 5657, 4907, 15504, 2033, 4907, 1059, 24475, 1045, 2106, 2025, 9040, 2017, 8418, 8528, 5657, 2129, 2079, 2017, 2066, 2068, 18108, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tdocument_level_task_label_ids: [0]\n",
            " \ttoken_level_task_label_ids: [0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset /content/hatexplain_train.csv: 100%|██████████| 11535/11535 [00:04<00:00, 2760.79 Dicts/s]\n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -   \n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -   LOADING DEV DATA\n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -   =================\n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -   Loading dev set from: /content/hatexplain_dev.csv\n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 3845 dictionaries to pytorch datasets (chunksize = 385)...\n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -   /w\\  /w\\\n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -   /'\\  /'\\\n",
            "05/31/2021 00:05:00 - INFO - farm.data_handler.data_silo -     \n",
            "Preprocessing Dataset /content/hatexplain_dev.csv:   0%|          | 0/3845 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "05/31/2021 00:05:01 - INFO - farm.data_handler.processor -   *** Show 1 random examples ***\n",
            "05/31/2021 00:05:01 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: None\n",
            "Clear Text: \n",
            " \ttext: catherine the only place to get white kids is from white women and the only way to overcome the invasion is white kids\n",
            " \tdocument_level_task_label: normal\n",
            " \ttoken_level_task_label: ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
            "Tokenized: \n",
            " \tNone\n",
            "Features: \n",
            " \tinput_ids: [101, 6615, 1996, 2069, 2173, 2000, 2131, 2317, 4268, 2003, 2013, 2317, 2308, 1998, 1996, 2069, 2126, 2000, 9462, 1996, 5274, 2003, 2317, 4268, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tdocument_level_task_label_ids: [0]\n",
            " \ttoken_level_task_label_ids: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset /content/hatexplain_dev.csv: 100%|██████████| 3845/3845 [00:01<00:00, 2604.76 Dicts/s]\n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -   \n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -   LOADING TEST DATA\n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -   =================\n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -   Loading test set from: /content/hatexplain_test.csv\n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 3844 dictionaries to pytorch datasets (chunksize = 385)...\n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -   /w\\  /w\\\n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -   / \\  / \\\n",
            "05/31/2021 00:05:02 - INFO - farm.data_handler.data_silo -     \n",
            "Preprocessing Dataset /content/hatexplain_test.csv:   0%|          | 0/3844 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "05/31/2021 00:05:03 - INFO - farm.data_handler.processor -   *** Show 1 random examples ***\n",
            "05/31/2021 00:05:03 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: None\n",
            "Clear Text: \n",
            " \ttext: all the black guys in green all look alike same muslim beards\n",
            " \tdocument_level_task_label: offensive\n",
            " \ttoken_level_task_label: ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1']\n",
            "Tokenized: \n",
            " \tNone\n",
            "Features: \n",
            " \tinput_ids: [101, 2035, 1996, 2304, 4364, 1999, 2665, 2035, 2298, 11455, 2168, 5152, 10154, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tdocument_level_task_label_ids: [1]\n",
            " \ttoken_level_task_label_ids: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset /content/hatexplain_test.csv: 100%|██████████| 3844/3844 [00:01<00:00, 2473.13 Dicts/s]\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   \n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   DATASETS SUMMARY\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   ================\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   Examples in train: 11535\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   Examples in dev  : 3845\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   Examples in test : 3844\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   \n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 29.820632856523623\n",
            "05/31/2021 00:05:04 - INFO - farm.data_handler.data_silo -   Proportion clipped:      8.669267446900737e-05\n",
            "05/31/2021 00:05:04 - INFO - farm.modeling.language_model -   \n",
            "05/31/2021 00:05:04 - INFO - farm.modeling.language_model -   LOADING MODEL\n",
            "05/31/2021 00:05:04 - INFO - farm.modeling.language_model -   =============\n",
            "05/31/2021 00:05:04 - INFO - farm.modeling.language_model -   Could not find bert-base-uncased locally.\n",
            "05/31/2021 00:05:04 - INFO - farm.modeling.language_model -   Looking on Transformers Model Hub (in local cache and online)...\n",
            "05/31/2021 00:05:10 - INFO - farm.modeling.language_model -   Loaded bert-base-uncased\n",
            "05/31/2021 00:05:10 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 3]\n",
            "05/31/2021 00:05:10 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 3]\n",
            "05/31/2021 00:05:11 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
            "05/31/2021 00:05:11 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "05/31/2021 00:05:11 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 36.1, 'num_training_steps': 361}'\n",
            "05/31/2021 00:05:11 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/0 (Cur. train loss: 152.8246): 100%|██████████| 361/361 [02:24<00:00,  2.50it/s]\n",
            "Evaluating: 100%|██████████| 121/121 [00:20<00:00,  5.78it/s]\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 361 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   \n",
            " _________ document_level_task _________\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   loss: 0.727332663957832\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   task_name: document_level_task\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   acc: 0.6909469302809573\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      normal     0.6896    0.8160    0.7475      1598\n",
            "   offensive     0.5833    0.4156    0.4854      1061\n",
            "  hatespeech     0.7611    0.7688    0.7649      1185\n",
            "\n",
            "    accuracy                         0.6909      3844\n",
            "   macro avg     0.6780    0.6668    0.6659      3844\n",
            "weighted avg     0.6823    0.6909    0.6805      3844\n",
            "\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   \n",
            " _________ token_level_task _________\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   loss: 4.2269659235871915\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   task_name: token_level_task\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   f1 macro score: 0.8104000582269297\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   total: 3844\n",
            "05/31/2021 00:07:58 - INFO - farm.eval -   report: \n",
            " Error\n",
            "05/31/2021 00:08:00 - INFO - farm.utils -   Using device: CUDA \n",
            "05/31/2021 00:08:00 - INFO - farm.utils -   Number of GPUs: 1\n",
            "05/31/2021 00:08:00 - INFO - farm.utils -   Distributed Training: False\n",
            "05/31/2021 00:08:00 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "05/31/2021 00:08:00 - INFO - farm.modeling.language_model -   \n",
            "05/31/2021 00:08:00 - INFO - farm.modeling.language_model -   LOADING MODEL\n",
            "05/31/2021 00:08:00 - INFO - farm.modeling.language_model -   =============\n",
            "05/31/2021 00:08:00 - INFO - farm.modeling.language_model -   Model found locally at /content/early-stopping-model\n",
            "05/31/2021 00:08:02 - INFO - farm.modeling.language_model -   Loaded /content/early-stopping-model\n",
            "05/31/2021 00:08:02 - INFO - farm.modeling.adaptive_model -   Found files for loading 2 prediction heads\n",
            "05/31/2021 00:08:02 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
            "05/31/2021 00:08:02 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 3]\n",
            "05/31/2021 00:08:02 - INFO - farm.modeling.prediction_head -   Loading prediction head from /content/early-stopping-model/prediction_head_0.bin\n",
            "05/31/2021 00:08:02 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
            "05/31/2021 00:08:02 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 3]\n",
            "05/31/2021 00:08:02 - INFO - farm.modeling.prediction_head -   Loading prediction head from /content/early-stopping-model/prediction_head_1.bin\n",
            "05/31/2021 00:08:02 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizerFast'\n",
            "05/31/2021 00:08:02 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
            "05/31/2021 00:08:02 - INFO - farm.utils -   Using device: CUDA \n",
            "05/31/2021 00:08:02 - INFO - farm.utils -   Number of GPUs: 1\n",
            "05/31/2021 00:08:02 - INFO - farm.utils -   Distributed Training: False\n",
            "05/31/2021 00:08:02 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "05/31/2021 00:08:02 - INFO - farm.infer -   Got ya 2 parallel workers to do inference ...\n",
            "05/31/2021 00:08:02 - INFO - farm.infer -    0    0 \n",
            "05/31/2021 00:08:02 - INFO - farm.infer -   /w\\  /w\\\n",
            "05/31/2021 00:08:02 - INFO - farm.infer -   /'\\  / \\\n",
            "05/31/2021 00:08:02 - INFO - farm.infer -     \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:101: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:101: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
            "05/31/2021 00:08:03 - INFO - farm.data_handler.processor -   *** Show 1 random examples ***\n",
            "05/31/2021 00:08:03 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: None\n",
            "Clear Text: \n",
            " \ttext: i got in trouble with the twitters for messing with bots what a fucking nigger\n",
            "Tokenized: \n",
            " \ttokens: ['[CLS]', 'i', 'got', 'in', 'trouble', 'with', 'the', 'twitter', '##s', 'for', 'messing', 'with', 'bot', '##s', 'what', 'a', 'fucking', 'ni', '##gger', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            " \tword_spans: [(0, 1), (2, 5), (6, 8), (9, 16), (17, 21), (22, 25), (26, 34), (35, 38), (39, 46), (47, 51), (52, 56), (57, 61), (62, 63), (64, 71), (72, 78)]\n",
            " \ttoken_to_word_map: [None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 10, 11, 12, 13, 14, 14, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Features: \n",
            " \tinput_ids: [101, 1045, 2288, 1999, 4390, 2007, 1996, 10474, 2015, 2005, 22308, 2007, 28516, 2015, 2054, 1037, 8239, 9152, 13327, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:12<00:00,  7.88 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.97 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.98 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  9.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  9.00 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.98 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.98 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.98 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 97/97 [00:10<00:00,  8.96 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 95/95 [00:10<00:00,  8.95 Batches/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Completed: seed_post42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuNbme_Vb05c"
      },
      "source": [
        "post_true_values = test_result_data.post_label.values\n",
        "token_true_values = test_result_data.toxic_tokens.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVyp1IdwcGm3"
      },
      "source": [
        "post_pred_values = []\n",
        "for idx in range(len(post_true_values)):\n",
        "  res_dict = {'offensive': 0, 'normal': 0, 'hatespeech': 0}\n",
        "\n",
        "  res_dict[test_result_data.seed_post3.values[idx]] += 1\n",
        "  res_dict[test_result_data.seed_post42.values[idx]] += 1\n",
        "\n",
        "  res_dict = {k: v for k, v in sorted(res_dict.items(), key=lambda item: -item[1])}\n",
        "\n",
        "  post_pred_values.append(list(res_dict)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Qwr7Upcaxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d08cc9-211f-48d3-bcd5-b8af9b638ad4"
      },
      "source": [
        "print(\"---- Post-level Results ----\")\n",
        "print(\"Seed 3:\", f1_score(post_true_values, test_result_data.seed_post3.values, average=\"macro\"))\n",
        "print(\"Seed 42:\", f1_score(post_true_values, test_result_data.seed_post42.values, average=\"macro\"))\n",
        "print(\"Overall (macro):\", f1_score(post_true_values, post_pred_values, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- Post-level Results ----\n",
            "Seed 3: 0.6729483068631462\n",
            "Seed 42: 0.6661457549112987\n",
            "Overall (macro): 0.6769352480006362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SSNo3lTdD7K"
      },
      "source": [
        "def res_customr_f1(y_true, y_pred):\n",
        "  f1_scores = []\n",
        "  idx = 0\n",
        "  for t, p in zip(y_true, y_pred):\n",
        "    try:\n",
        "      t = ast.literal_eval(t)\n",
        "      cur = f1_score(t, p, average='macro')\n",
        "      f1_scores.append(cur)\n",
        "    except Exception as e:\n",
        "      diff = len(t) - len(p)\n",
        "      p = p + [0] * diff\n",
        "      cur = f1_score(t, p, average='macro')\n",
        "      f1_scores.append(cur)\n",
        "    idx += 1\n",
        "  return \"Mean F1 (macro) score: \" + str(sum(f1_scores) / len(f1_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAR_IVIYdHTc"
      },
      "source": [
        "def majority_vote(results_df, random_seed_list):\n",
        "  pred_list = []\n",
        "  for idx in range(len(results_df)):\n",
        "    indv_list = []\n",
        "    for seed in random_seed_list:\n",
        "      seed_name = \"seed_token\" + str(seed)\n",
        "      seed_list = results_df[seed_name].values[idx]\n",
        "      if len(indv_list) == 0:\n",
        "        for i in range(len(seed_list)):\n",
        "          indv_list.append(dict({0:0, 1:0}))\n",
        "      for idx_sl, idv_tokens in enumerate(seed_list):\n",
        "        indv_list[idx_sl][idv_tokens] += 1\n",
        "    fresh_list = []\n",
        "    for token_dict in indv_list:\n",
        "      token_dict = {k: v for k, v in sorted(token_dict.items(), key=lambda item: -item[1])}\n",
        "      fresh_list.append(list(token_dict)[0])\n",
        "    pred_list.append(fresh_list)\n",
        "  return pred_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcc-KCC2dI5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b158c760-e71b-4598-d627-901ce5eb4b04"
      },
      "source": [
        "print(\"---- Token-level Results ----\")\n",
        "print(\"Seed 3:\", res_customr_f1(token_true_values, test_result_data.seed_token3.values))\n",
        "print(\"Seed 42:\", res_customr_f1(token_true_values, test_result_data.seed_token42.values))\n",
        "print(\"Overall Mean:\", res_customr_f1(token_true_values, majority_vote(test_result_data, random_seed_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- Token-level Results ----\n",
            "Seed 3: Mean F1 (macro) score: 0.8078636209838234\n",
            "Seed 42: Mean F1 (macro) score: 0.8110865138957913\n",
            "Overall Mean: Mean F1 (macro) score: 0.8122438285971888\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}